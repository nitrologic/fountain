{
	"tts-1@openai":{
		"pricing":[0.60,12.0],
		"purpose":"tts $4 per 1000 seconds",
		"endpoints":["v1/audio/speech"]
	},
	"gpt-4o-mini-tts@openai":{
		"pricing":[0.60,12.0],
		"purpose":"gpt-5",
		"endpoints":["v1/audio/speech"]
	},
	"gpt-5-2025-08-07@openai":{
		"pricing":[1.25,0.125,10.0],
		"purpose":"gpt-5"
	},
	"gpt-5-mini-2025-08-07@openai":{
		"pricing":[0.25,0.025,2.0],
		"purpose":"gpt-5 mini"
	},
	"gpt-5-nano-2025-08-07@openai":{
		"pricing":[0.05,0.025,0.20],
		"purpose":"nano nano 5 - faster than a fast thing",
		"press":"supporting up to ? million tokens of context",
		"reality":"hopefully does not suffer from clueless ideation"
	},
	"deepseek-chat@deepseek": {
		"pricing": [0.27, 0.07, 1.10],
		"released": "2025-03-24",
		"purpose": "V3-0324 General conversation and text generation",
		"press": "Cost-effective model for versatile tasks, trained on massive text datasets."
	},
	"deepseek-reasoner@deepseek": {
		"pricing": [0.55, 0.14, 2.19],
		"released": "2025-05-28",
		"purpose": "R1-0528 Advanced reasoning and problem-solving",
		"press": "High-performance model for complex math, coding, and logical tasks."
	},
	"meta-llama/Meta-Llama-3.1-405B-Instruct@hyperbolic":{
		"pricing": [0.55, 2.19],
		"purpose": "405B Multimodal",
		"released": "2024-07-23"
	},
	"Qwen/Qwen2.5-VL-72B-Instruct@hyperbolic":{
		"pricing": [0.55, 0.14, 2.19],
		"purpose": "72B Multimodal",
		"released": "2024-09-12"
	},
	"llama-3.3-70b@cerebras":{
		"pricing": [1.60,6.40],
		"purpose": "happyface cerebras hosted llama 3.3",
		"released": "2024-12-06"
	},
	"llama-4-maverick-17b-128e-instruct@cerebras":{
		"pricing": [1.60,6.40],
		"purpose": "happyface cerebras hosted llama 4 maverick",
		"released": "2025-03-09"
	},
	"llama-4-scout-17b-16e-instruct@cerebras":{
		"pricing": [1.60,6.40],
		"purpose": "happyface cerebras hosted llama 4 scout",
		"released": "2025-03-09"
	},
	"llama3.1-8b@cerebras":{
		"pricing": [1.60,6.40],
		"purpose": "happyface cerebras hosted llama 3.1",
		"released": "2024-07-23"
	},
	"qwen-3-32b@cerebras":{
		"pricing": [1.60,6.40],
		"purpose": "happyface cerebras hosted qwen 3",
		"released": "2025-05-15"
	},
	"qwen-max@alibaba":{
		"pricing": [1.60,6.40],
		"purpose":"Flagship model, complex reasoning, coding, and multimodal tasks."
	},
	"qwen-plus@alibaba":{
		"pricing":[0.40,1.20],
		"purpose":"Balanced performance, speed and cost."
	},
	"qwen-turbo@alibaba":{
		"pricing":[0.05,0.20],
		"purpose":"Fast speed and low cost"
	},
	"qwen2-7b-instruct@alibaba":{
		"pricing":[0.175,0.70],
		"multi":true,
		"purpose":"Open source Qwen2 model, failing multi image test"
	},
	"o3-2025-04-16@openai":{
		"pricing":[2.00, 0.50, 8.00],
		"purpose":"well-rounded and powerful model across domains",
		"press":"Excels at math, science, coding, visual reasoning tasks, technical writing and instruction-following"
	},
	"o3-mini-2025-01-31@openai":{
		"pricing": [10.00, 2.50, 40.00],
		"purpose": "Pushing the frontier of cost-effective reasoning.",
		"press": "tbc",
		"reality": "fabricates often, cuts corners oddly"
	},
	"gpt-4o-2024-08-06@openai": {
		"pricing": [3.75, 1.875, 15.00],
		"purpose": "General use with fine-tuning capabilities",
		"press": "Designed to handle complex queries",
		"reality": "Full featured model"
	},
	"gpt-4o-mini-2024-07-18@openai": {
		"pricing": [0.30, 0.15, 1.20],
		"purpose": "Lightweight model for efficiency",
		"press": "Optimized for speed over complexity",
		"reality": "Reduced capacity model"
	},
	"gpt-4.1-2025-04-14@openai":{
		"pricing":[2.00,0.50,8.00],
		"purpose":"Smartest model for complex tasks,",
		"press":"GPT models for everyday tasks"
	},
	"gpt-4.1-nano-2025-04-14@openai":{
		"pricing":[0.10,0.025,0.40],
		"purpose":"nano nano - faster than a fast thing",
		"press":"supporting up to 1 million tokens of context",
		"reality":"suffers from clueless ideation"
	},
	"gpt-4.1-mini-2025-04-14@openai":{
		"pricing":[0.40,0.10,1.60],
		"multi":true,
		"purpose":"image and text support",
		"press":"a million tokens of context"
	},
	"o4-mini-2025-04-16@openai":{
		"pricing":[1.10,0.275,4.40],
		"cold":true,
		"multi":true,
		"purpose":"optimized for fast, cost-efficient reasoning",
		"press":"remarkable performance for its size and cost, particularly in math, coding, and visual tasks",
		"reality":"a real unit, literally no warmth, single temp"
	},
	"gpt-4.5-preview-2025-02-27@openai":{
		"pricing":[75.00,37.50,150.00],
		"purpose":"burn money"
	},
	"models/gemini-2.5-flash-preview-tts@gemini":{
		"endpoints":["v1/audio/speech"],
		"released":"2025-05-20",
		"pricing":[0.50,10.0]
	},
	"models/gemini-2.5-pro-preview-tts@gemini":{
		"endpoints":["v1/audio/speech"],
		"released":"2025-05-20",
		"pricing":[1.00,20.0]
	},
	"models/gemini-2.5-pro-preview-06-05@gemini":{
		"pricing":[1.25,10.0],
		"released":"2025-06-05",
		"purpose":"Excels at coding and complex reasoning tasks",
		"press":"A state-of-the-art multipurpose model",
		"reality":"Not cheap, and often a complete dunce."
	},
	"models/gemini-2.5-flash-preview-05-20@gemini":{
		"pricing":[0.15,0.15,0.60,3.50],
		"released":"2025-05-20",
		"purpose":"Adaptive thinking, cost efficiency.",
		"press":"The best price-to-performance ratio.",
		"reality":"testing now, thinking budget say what now?"
	},
	"models/gemini-2.0-flash@gemini":{
		"pricing":[0.10,0.40],
		"released": "2024-12-11",
		"purpose":"High-performance multimodal model optimized for agent-based applications.",
		"press":"Balanced performance across tasks with a 1 million token context window, built for the era of Agents.",
		"reality":"A tendency to regurgitate Mr Creosote style."
	},
	"grok-code-fast-1@xai":{
		"pricing": [0.20, 1.50],
		"released": "2025-08-20",
		"purpose":"256K context window."
	},
	"grok-2-vision-1212@xai":{
		"pricing": [2.00, 0.75, 10.00],
		"multi": true,
		"purpose": "Image vision, text and code generation",
		"reality": "Model under test."
	},
	"grok-4-0709@xai":{
		"pricing": [3.00, 15.00],
		"multi":true,
		"maxprompt": 256000,
		"purpose": "Advanced reasoning and code generation",
		"press": "High-performance model for complex research tasks and multi-step workflows."
	},
	"grok-3@xai": {
		"pricing": [3.00, 0.75, 15.00],
		"purpose": "Text and code generation",
		"press": "Versatile model balancing performance and cost for diverse applications.",
		"reality": "Fair, thorough, sadly a tad expensive, oops, maximum prompt length exceeded."
	},
	"grok-3-fast@xai": {
		"pricing": [5.00, 1.25, 25.00],
		"purpose": "Rapid text generation and reasoning",
		"press": "High-speed model for time-sensitive tasks and efficient processing."
	},
	"grok-3-mini@xai": {
		"pricing": [0.30, 0.075, 0.50],
		"purpose": "Efficient text generation",
		"press": "Compact model for lightweight reasoning and cost-effective tasks.",
		"reality": "128K token limit for nice and cheap, same positive vibe as big brother"
	},
	"grok-3-mini-fast@xai": {
		"pricing": [0.60, 0.15, 4.00],
		"purpose": "Fast text generation",
		"press": "Lightweight, high-speed model for basic reasoning and cost-efficient applications."
	},
	"grok-2-1212@xai": {
		"pricing": [2.00, 10.00],
		"purpose": "Advanced reasoning and code generation",
		"press": "High-performance model for complex research tasks and multi-step workflows."
	},
	"codestral-2411-rc5@mistral":{
		"pricing":[0.5,1.50],
		"strict":true,
		"purpose":"262,144 token low latency coding"
	},
	"mistral-small-2506@mistral":{
		"pricing":[0.50,1.50],
		"strict":true,
		"purpose":"lightweight multilingual reasoning"
	},
	"mistral-ocr-2505@mistral":{
		"pricing":[0.50,1.50],
		"strict":true,
		"ocr":true,
		"purpose":"50MB per page analyser",
		"press": "High-performance model for complex research tasks and multi-step workflows."
	},
	"claude-opus-4-1-20250805@anthropic":{
		"pricing":[15.0,75.0],
		"limit":[20000],
		"purpose":"frontier model for coding, agentic search, and creative writing",
		"reality":"rate limited 20,000 input tokens per minute"
	},
	"claude-opus-4-20250514@anthropic":{
		"pricing":[15.0,75.0],
		"limit":[20000],
		"purpose":"frontier model for coding, agentic search, and creative writing",
		"reality":"rate limited 20,000 input tokens per minute"
	},
	"claude-sonnet-4-20250514@anthropic":{
		"pricing":[3.0,15.0],
		"purpose":"balance performance, cost-effectiveness, and speed for high-volume production"
	},
	"claude-3-haiku-20240307@anthropic":{
		"pricing":[0.80,4.0],
		"purpose":"fast speeds, improved instruction following, and more accurate tool use",
		"reality":"I do not feel comfortable providing or discussing blah blah."
	},
	"moonshot-v1-128k-vision-preview@moonshot": {
		"pricing": [2,5],
		"multi": true,
		"purpose": "High-res image recognition and analysis, 128K context window",
		"press": "High-resolution image analysis for detailed recognition tasks."
	},
	"moonshot-v1-8k@moonshot": {
		"pricing": [0.20,0.15,2.0],
		"purpose": "Efficient real-time text processing, 8K context window",
		"press": "Designed for efficient, real-time text processing."
	},
	"moonshot-v1-32k@moonshot": {
		"pricing": [1.00,0.15,3.0],
		"purpose": "Advanced text analysis, 32K context window",
		"press": "Balancing performance and context for advanced text analysis."
	},
	"moonshot-v1-128k@moonshot": {
		"pricing": [2.00,0.15,5.0],
		"purpose": "Comprehensive text understanding, 128K context window",
		"press": "Ideal for deep learning applications requiring comprehensive text understanding."
	},
	"kimi-k2-0711-preview@moonshot": {
		"pricing": [0.60,0.15,2.5],
		"purpose": "Mixture of Experts model, 128K context window",
		"press": "Enhanced accuracy through a Mixture of Experts approach."
	},
	"kimi-k2-turbo-preview@moonshot":{
		"pricing": [1.20,0.30,5.0],
		"purpose": "Faster thand ever, 128K context window"
	},
	"meta-llama/Llama-3.1-8B-Instruct@nscale":{
		"pricing":[0.88,0.15,2.5],
		"purpose":"zuk llama",
		"released": "2024-07-23"
	}
}
